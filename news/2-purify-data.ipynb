{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we'll use an RNN to identify domain-specific tokens that allow the model to cheat, and\n",
    "remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, log_loss\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import utils\n",
    "import rnn\n",
    "\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.52 s, sys: 28 ms, total: 1.54 s\n",
      "Wall time: 1.54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "filepath = '../data/title-1-True.pkl'\n",
    "\n",
    "df, keep_stops, min_sents, vocab, w2i, i2w = utils.read_dataset(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limit vocab size, pad sequences, and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxlen = 20        # Max number of tokens in input sequence\n",
    "topn = 6747        # Keep only the top n words in vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 152 ms, sys: 8 ms, total: 160 ms\n",
      "Wall time: 153 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Filter out uncommon words.\n",
    "\n",
    "df['encoded_text'] = utils.filter_top_words(df['encoded_text'].tolist(), topn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = pad_sequences(df['encoded_text'], maxlen=maxlen, value=topn,  \n",
    "                  padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lb = LabelBinarizer()\n",
    "y = lb.fit_transform(df['encoded_domain'])\n",
    "\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Delete the dataframe, we are done with it for now!\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model\n",
    "\n",
    "This is an important part, so I'll be explicit here rather than hiding things in `utils :)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed_dim = 256          # embedding dimension for word vecs\n",
    "num_gru = 1              # number of GRUs to use in serial\n",
    "gru_dim = 256            # dimension of GRU layers\n",
    "gru_activation = 'sigmoid'  # activation function for GRU layer\n",
    "bidirectional = False    # whether to use bidirectional\n",
    "dense_dim = 256          # dimensionality of dense layer\n",
    "dropout = 0.5            # dropout ratio\n",
    "batch_size = 64          # batch size\n",
    "validation_split = 0.1   # Fraction of samples to keep out for validation\n",
    "max_epochs = 50          # maximum number of epochs to run for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.0975417378730117,\n",
       " 1: 0.11055606678059682,\n",
       " 2: 0.49152096752990665,\n",
       " 3: 0.014328907585118969,\n",
       " 4: 0.6015512028394899,\n",
       " 5: 0.41869330879453137,\n",
       " 6: 1.0,\n",
       " 7: 0.2020507427369528,\n",
       " 8: 0.32023136584724593,\n",
       " 9: 0.0984619429472854,\n",
       " 10: 0.5774944130406204,\n",
       " 11: 0.15958985145260943,\n",
       " 12: 0.1125279347968976,\n",
       " 13: 0.06441435519915867,\n",
       " 14: 0.10148547390561326,\n",
       " 15: 0.33258840541606416,\n",
       " 16: 0.15038780070987248,\n",
       " 17: 0.4878401472328119,\n",
       " 18: 0.26317865124227685,\n",
       " 19: 0.19232286052320233,\n",
       " 20: 0.30971473642697517,\n",
       " 21: 0.08847114499802813,\n",
       " 22: 0.16550545550151177,\n",
       " 23: 0.10674378861574865,\n",
       " 24: 0.10135401603785986,\n",
       " 25: 0.6331010911003023,\n",
       " 26: 0.2681740502169055,\n",
       " 27: 0.5297752070461417,\n",
       " 28: 0.2250558695937952,\n",
       " 29: 0.19666097015906403,\n",
       " 30: 0.2576574207966347,\n",
       " 31: 0.08373866175890628}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Classes are pretty imbalanced, so let's balance them out\n",
    "\n",
    "weights = []\n",
    "for i in lb.classes_:\n",
    "    weights.append(len(np.where(np.argmax(y, axis=1) == i)[0]))\n",
    "    \n",
    "max_weight = max(weights)\n",
    "weights = [1.*x/max_weight for x in weights]\n",
    "class_weight = {k: w for k, w in zip(lb.classes_, weights)}\n",
    "\n",
    "class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving checkpoints to: models/256_1_256_256_0.5_False_20_6747_64_32_sigmoid_{epoch:03d}_{val_loss:.5f}_{val_acc:.5f}_titles.h5\n"
     ]
    }
   ],
   "source": [
    "# Let's save our best current checkpoints, and stop if we haven't improved in 3 iterations w.r.t. val_acc.\n",
    "\n",
    "model_dir = 'models'\n",
    "basename = '{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}'.format(gru_dim, num_gru, embed_dim, dense_dim,  \n",
    "                dropout, bidirectional, maxlen, topn, batch_size, len(lb.classes_), gru_activation) \n",
    "filepath = os.path.join(model_dir, basename + '_{epoch:03d}_{val_loss:.5f}_{val_acc:.5f}_titles.h5')\n",
    "\n",
    "print('saving checkpoints to: {}'.format(filepath))\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True, \n",
    "                                   save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 20, 256)           1727488   \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 256)               393984    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                8224      \n",
      "=================================================================\n",
      "Total params: 2,195,488\n",
      "Trainable params: 2,195,488\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = rnn.get_training_model(topn, embed_dim, dense_dim, gru_dim, num_gru, maxlen, dropout,\n",
    "                               bidirectional, len(lb.classes_), gru_activation)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 59992 samples, validate on 6666 samples\n",
      "Epoch 1/50\n",
      "59992/59992 [==============================] - 26s - loss: 1.0102 - acc: 0.2247 - val_loss: 0.8256 - val_acc: 0.3252\n",
      "Epoch 2/50\n",
      "59992/59992 [==============================] - 26s - loss: 0.7985 - acc: 0.3463 - val_loss: 0.7441 - val_acc: 0.3740\n",
      "Epoch 3/50\n",
      "59992/59992 [==============================] - 26s - loss: 0.7267 - acc: 0.3929 - val_loss: 0.7363 - val_acc: 0.4007\n",
      "Epoch 4/50\n",
      "59992/59992 [==============================] - 26s - loss: 0.6845 - acc: 0.4209 - val_loss: 0.7296 - val_acc: 0.4055\n",
      "Epoch 5/50\n",
      "59992/59992 [==============================] - 26s - loss: 0.6544 - acc: 0.4447 - val_loss: 0.7365 - val_acc: 0.4035\n",
      "Epoch 6/50\n",
      "59992/59992 [==============================] - 26s - loss: 0.6320 - acc: 0.4600 - val_loss: 0.7425 - val_acc: 0.4109\n",
      "Epoch 7/50\n",
      "59992/59992 [==============================] - 26s - loss: 0.6111 - acc: 0.4744 - val_loss: 0.7427 - val_acc: 0.4125\n",
      "Epoch 8/50\n",
      "59992/59992 [==============================] - 26s - loss: 0.5900 - acc: 0.4868 - val_loss: 0.7581 - val_acc: 0.4142\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X, y, epochs=max_epochs, validation_split=validation_split,\n",
    "                 callbacks=[model_checkpoint, early_stopping],\n",
    "                 class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use model to remove \"hints\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start by reloading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.55 s, sys: 20 ms, total: 1.57 s\n",
      "Wall time: 1.58 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Load the source data frame\n",
    "\n",
    "filepath = '../data/title-1-True.pkl'\n",
    "\n",
    "df, keep_stops, min_sents, vocab, w2i, i2w = utils.read_dataset(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 128 ms, sys: 12 ms, total: 140 ms\n",
      "Wall time: 121 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Filter out uncommon words using parameters parsed from model name.\n",
    "\n",
    "df['encoded_text'] = utils.filter_top_words(df['encoded_text'].tolist(), topn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create input/output data\n",
    "\n",
    "X = pad_sequences(df['encoded_text'], maxlen=maxlen, value=topn, \n",
    "                  padding='post', truncating='post')\n",
    "\n",
    "Y = df['encoded_domain'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.24 s, sys: 612 ms, total: 1.85 s\n",
      "Wall time: 1.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_name = 'models/256_1_256_256_0.5_False_20_6747_64_32_sigmoid_003_0.72958_0.40549_titles.h5'\n",
    "\n",
    "# Get model params from name\n",
    "gru_dim, num_gru, embed_dim, dense_dim, dropout, bidirectional, maxlen, topn, batch_size, output_size, gru_activation = \\\n",
    "  utils.parse_model_name(model_name)\n",
    "\n",
    "# Load the keras model\n",
    "model = load_model(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the model so you can predict token by token\n",
    "\n",
    "in_model, out_model = rnn.split_model_layers(model, topn, embed_dim, dense_dim, gru_dim, num_gru, maxlen, \n",
    "  output_size, bidirectional, gru_activation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify domains with lots of \"suspicious\", ie. very high confidence, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get output probabilities for every observation.\n",
    "\n",
    "Ypred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a column with the highest prediction\n",
    "\n",
    "df['predicted_prob'] = np.max(Ypred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66560/66658 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "# Create a column with the predicted domain\n",
    "\n",
    "df['predicted_domain'] = model.predict_classes(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16204, 10)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pull out high-probability samples\n",
    "\n",
    "hi = pd.DataFrame(df[df['predicted_prob'] > 0.8])\n",
    "hi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ap.org</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>americanthinker.com</th>\n",
       "      <td>0.998652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>realclearpolitics.com</th>\n",
       "      <td>0.997003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rightwingnews.com</th>\n",
       "      <td>0.990662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>westernjournalism.com</th>\n",
       "      <td>0.619388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>breitbart.com</th>\n",
       "      <td>0.605777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>go.com</th>\n",
       "      <td>0.428636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn.com</th>\n",
       "      <td>0.318785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>washingtonpost.com</th>\n",
       "      <td>0.266253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nytimes.com</th>\n",
       "      <td>0.126650</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         domain\n",
       "ap.org                 1.000000\n",
       "americanthinker.com    0.998652\n",
       "realclearpolitics.com  0.997003\n",
       "rightwingnews.com      0.990662\n",
       "westernjournalism.com  0.619388\n",
       "breitbart.com          0.605777\n",
       "go.com                 0.428636\n",
       "cnn.com                0.318785\n",
       "washingtonpost.com     0.266253\n",
       "nytimes.com            0.126650"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at domains that have the highest probabilities\n",
    "\n",
    "conf_thresh = 0.10  # threshold for how many high-confidence samples a domain has\n",
    "\n",
    "counts_hi = pd.DataFrame(hi['domain'].value_counts(), dtype=float) \n",
    "counts_all = pd.DataFrame(df['domain'].value_counts())\n",
    "\n",
    "# Compute the ratio of hi-conf\n",
    "for row in counts_hi.iterrows():\n",
    "    counts_hi.ix[row[0]] /= counts_all.ix[row[0]]\n",
    "\n",
    "# Sort by percent of hi-conf and truncate at threshold\n",
    "counts_hi = counts_hi.sort_values('domain', ascending=False)\n",
    "counts_hi = counts_hi[counts_hi['domain'] > conf_thresh]\n",
    "counts_hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the model to identify tokens used to \"cheat\" and create blacklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_hival_tokens(domain, conf_thresh=0.9, token_thresh=0.9):\n",
    "    ''' Returns a dictionary of tokens that occur almost always in a specific domain.\n",
    "    These are considered hints that the model is using to solve the bias classification\n",
    "    problem, and should be removed.\n",
    "    \n",
    "    I'm being a little sloppy here and pulling X df, *_model from global scope :P\n",
    "    \n",
    "    Args:\n",
    "        domain_thresh: threshold above which a domain prediction is considered high-confidence\n",
    "        token_thresh: threshold for considering a token to have a high confidence\n",
    "    '''\n",
    "\n",
    "    # Extract the specified domain\n",
    "    hi = pd.DataFrame(df[df['domain'] == domain])\n",
    "\n",
    "    # Only consider rows where the prediction was correct, i.e., cheating may have happened\n",
    "    hi = pd.DataFrame(hi[hi['encoded_domain'] == hi['predicted_domain']])\n",
    "    \n",
    "    # Select only high-confidence samples\n",
    "    hi = pd.DataFrame(hi[hi['predicted_prob'] > conf_thresh])\n",
    "  \n",
    "    # For each observation\n",
    "    tokens = defaultdict(float)\n",
    "    for row in hi.iterrows():\n",
    "        idx = np.where(df.index == row[0])[0][0]\n",
    "        P = rnn.sequential_pred_for_class(X, df, idx, in_model, out_model)\n",
    "        T = df['tokenized'].iloc[idx].split()\n",
    "        hival_idx = np.where(P > token_thresh)[0]\n",
    "        for i in hival_idx:\n",
    "            if i >= len(T):\n",
    "                break\n",
    "            tokens[T[i]] += 1\n",
    "  \n",
    "    # Normalize the token counts as probabilities\n",
    "    for k in tokens:\n",
    "        tokens[k] = 1. * tokens[k] / hi.shape[0]\n",
    "  \n",
    "    return hi, tokens\n",
    "\n",
    "def create_blacklist(conf_thresh=0.2, domain_prob=0.9, token_thresh=0.9, count_thresh=0.1):\n",
    "    '''This function returns a blacklist dictionary indicating which tokens are allowing\n",
    "    the model to cheat for each domain.\n",
    "    \n",
    "    Args:\n",
    "        conf_thresh: threshold for considering a prediction high-confidence\n",
    "        domain_prob: threshold for considering a domain to have a high number of hi-conf predictions\n",
    "        token_thresh: threshold for fraction of articles containing token\n",
    "        count_thresh: threshold for number of times a token must \n",
    "    '''\n",
    "    \n",
    "    # Get rows corresponding to high predicted probability\n",
    "    hi = pd.DataFrame(df[df['predicted_prob'] > conf_thresh])\n",
    "\n",
    "    # Get fraction of high-confidence samples per domain\n",
    "    counts_hi = pd.DataFrame(hi['domain'].value_counts(), dtype=float)\n",
    "    counts_all = pd.DataFrame(df['domain'].value_counts())\n",
    "    for row in counts_hi.iterrows():\n",
    "        counts_hi.ix[row[0]] /= counts_all.ix[row[0]]\n",
    "\n",
    "    # Sort the count values by domain\n",
    "    counts_hi = counts_hi.sort_values('domain', ascending=False)\n",
    "    counts_hi = counts_hi[counts_hi['domain'] > domain_prob]\n",
    "\n",
    "    # For each domain, identify tokens that give the model high confidence\n",
    "    blacklist = {}\n",
    "    for domain in counts_hi.index:\n",
    "        print(domain)\n",
    "        hi, tokens = get_hival_tokens(domain, conf_thresh, token_thresh)\n",
    "        counts = [(v, k) for k, v in tokens.iteritems()]\n",
    "        blacklist[domain] = [x[1] for x in counts if x[0] >= count_thresh]\n",
    "        print('  {}'.format(blacklist[domain]))\n",
    "\n",
    "    return blacklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "americanthinker.com\n",
      "  [u'and_CCONJ', u'in_ADP', u'-PRON-_PRON', u'article_NOUN', u'the_DET', u'a_DET', u'of_ADP', u'for_ADP', u'trump_PROPN', u'on_ADP', u'to_ADP', u'be_VERB', u'to_PART', u'obama_PROPN']\n",
      "ap.org\n",
      "  [u'press_PROPN', u'associated_PROPN']\n",
      "realclearpolitics.com\n",
      "  [u'realclearpolitics_PROPN', u'realclearpolitic_NOUN']\n",
      "rightwingnews.com\n",
      "  [u'news_PROPN', u'right_PROPN', u'hawkins_PROPN', u'wing_PROPN', u'john_PROPN']\n",
      "cnn.com\n",
      "  [u'video_PROPN', u'cnn_PROPN']\n",
      "breitbart.com\n",
      "  [u'breitbart_PROPN']\n",
      "nytimes.com\n",
      "  []\n",
      "westernjournalism.com\n",
      "  [u'with_ADP', u'in_ADP', u'-PRON-_PRON', u'do_VERB', u'news_PROPN', u'the_DET', u'instantly_ADV', u'announcement_NOUN', u'trump_PROPN', u'to_ADP', u'-PRON-_ADJ', u'be_VERB', u'just_ADV']\n",
      "wsj.com\n",
      "  []\n",
      "theatlantic.com\n",
      "  []\n",
      "washingtonpost.com\n",
      "  [u'and_CCONJ', u'in_ADP', u'-PRON-_PRON', u'opinion_NOUN', u'the_DET', u'a_DET', u'of_ADP', u'for_ADP', u'perspective_NOUN', u'trump_PROPN', u'-PRON-_ADJ', u'be_VERB', u'to_PART']\n",
      "usatoday.com\n",
      "  []\n",
      "newsmax.com\n",
      "  []\n",
      "go.com\n",
      "  [u'and_CCONJ', u'with_ADP', u'in_ADP', u'the_DET', u'after_ADP', u'a_DET', u'of_ADP', u'video_NOUN', u'at_ADP', u'for_ADP', u'trump_PROPN', u'on_ADP', u'to_ADP', u'-PRON-_ADJ', u'be_VERB', u'to_PART', u'president_PROPN']\n",
      "cnbc.com\n",
      "  []\n",
      "weeklystandard.com\n",
      "  [u'prufrock_NOUN', u'the_DET']\n"
     ]
    }
   ],
   "source": [
    "blacklist = create_blacklist(count_thresh=0.05, token_thresh=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'americanthinker.com': [u'and_CCONJ',\n",
       "  u'in_ADP',\n",
       "  u'-PRON-_PRON',\n",
       "  u'article_NOUN',\n",
       "  u'the_DET',\n",
       "  u'a_DET',\n",
       "  u'of_ADP',\n",
       "  u'for_ADP',\n",
       "  u'trump_PROPN',\n",
       "  u'on_ADP',\n",
       "  u'to_ADP',\n",
       "  u'be_VERB',\n",
       "  u'to_PART',\n",
       "  u'obama_PROPN'],\n",
       " u'ap.org': [u'press_PROPN', u'associated_PROPN'],\n",
       " u'breitbart.com': [u'breitbart_PROPN'],\n",
       " u'cnbc.com': [],\n",
       " u'cnn.com': [u'video_PROPN', u'cnn_PROPN'],\n",
       " u'go.com': [u'and_CCONJ',\n",
       "  u'with_ADP',\n",
       "  u'in_ADP',\n",
       "  u'the_DET',\n",
       "  u'after_ADP',\n",
       "  u'a_DET',\n",
       "  u'of_ADP',\n",
       "  u'video_NOUN',\n",
       "  u'at_ADP',\n",
       "  u'for_ADP',\n",
       "  u'trump_PROPN',\n",
       "  u'on_ADP',\n",
       "  u'to_ADP',\n",
       "  u'-PRON-_ADJ',\n",
       "  u'be_VERB',\n",
       "  u'to_PART',\n",
       "  u'president_PROPN'],\n",
       " u'newsmax.com': [],\n",
       " u'nytimes.com': [],\n",
       " u'realclearpolitics.com': [u'realclearpolitics_PROPN',\n",
       "  u'realclearpolitic_NOUN'],\n",
       " u'rightwingnews.com': [u'news_PROPN',\n",
       "  u'right_PROPN',\n",
       "  u'hawkins_PROPN',\n",
       "  u'wing_PROPN',\n",
       "  u'john_PROPN'],\n",
       " u'theatlantic.com': [],\n",
       " u'usatoday.com': [],\n",
       " u'washingtonpost.com': [u'and_CCONJ',\n",
       "  u'in_ADP',\n",
       "  u'-PRON-_PRON',\n",
       "  u'opinion_NOUN',\n",
       "  u'the_DET',\n",
       "  u'a_DET',\n",
       "  u'of_ADP',\n",
       "  u'for_ADP',\n",
       "  u'perspective_NOUN',\n",
       "  u'trump_PROPN',\n",
       "  u'-PRON-_ADJ',\n",
       "  u'be_VERB',\n",
       "  u'to_PART'],\n",
       " u'weeklystandard.com': [u'prufrock_NOUN', u'the_DET'],\n",
       " u'westernjournalism.com': [u'with_ADP',\n",
       "  u'in_ADP',\n",
       "  u'-PRON-_PRON',\n",
       "  u'do_VERB',\n",
       "  u'news_PROPN',\n",
       "  u'the_DET',\n",
       "  u'instantly_ADV',\n",
       "  u'announcement_NOUN',\n",
       "  u'trump_PROPN',\n",
       "  u'to_ADP',\n",
       "  u'-PRON-_ADJ',\n",
       "  u'be_VERB',\n",
       "  u'just_ADV'],\n",
       " u'wsj.com': []}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blacklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter out cheat words and re-encode text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dg = df.copy()\n",
    "#df = dg.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for domain in blacklist.keys():\n",
    "    domain_index = df[df['domain'] == domain].index\n",
    "    data = df.ix[domain_index, 'tokenized'].tolist()\n",
    "    for ix, d in enumerate(data):\n",
    "        data[ix] = ' '.join([x for x in d.split() if x not in blacklist[domain]])\n",
    "    df.loc[domain_index, 'tokenized'] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['encoded_text'] = df['tokenized'].map(lambda x: [w2i[y] for y in x.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote to ../data/title-1-True-clean.pkl\n",
      "CPU times: user 1.8 s, sys: 36 ms, total: 1.84 s\n",
      "Wall time: 1.83 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "OUTPUT_FILE = '../data/title-{}-{}-clean.pkl'.format(min_sents, keep_stops)\n",
    "\n",
    "_ = utils.write_dataset(OUTPUT_FILE, df, keep_stops, min_sents, vocab, w2i, i2w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
