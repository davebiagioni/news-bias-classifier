{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we'll do further processing of the input text (in the spirit of hyperparameter tuning, rather than cleaning etc).  We'll then build and train a simple RNN classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, log_loss\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import utils\n",
    "import rnn\n",
    "\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.52 s, sys: 40 ms, total: 1.56 s\n",
      "Wall time: 1.56 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "filepath = '../data/title-1-True.pkl'\n",
    "\n",
    "df, keep_stops, min_sents, vocab, w2i, i2w = utils.read_dataset(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limit vocab size, pad sequences, and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxlen = 20        # Max number of tokens in input sequence\n",
    "topn = 6747        # Keep only the top n words in vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 160 ms, sys: 44 ms, total: 204 ms\n",
      "Wall time: 162 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Filter out uncommon words.\n",
    "\n",
    "df['encoded_text'] = utils.filter_top_words(df['encoded_text'].tolist(), topn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = pad_sequences(df['encoded_text'], maxlen=maxlen, value=topn,  \n",
    "                  padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66658, 32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb = LabelBinarizer()\n",
    "y = lb.fit_transform(df['encoded_domain'])\n",
    "\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Delete the dataframe, we are done with it for now!\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model\n",
    "\n",
    "This is an important part, so I'll be explicit here rather than hiding things in `utils :)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed_dim = 256          # embedding dimension for word vecs\n",
    "num_gru = 1              # number of GRUs to use in serial\n",
    "gru_dim = 256            # dimension of GRU layers\n",
    "gru_activation = 'sigmoid'  # activation function for GRU layer\n",
    "bidirectional = False    # whether to use bidirectional\n",
    "dense_dim = 256          # dimensionality of dense layer\n",
    "dropout = 0.5            # dropout ratio\n",
    "batch_size = 64          # batch size\n",
    "validation_split = 0.1   # Fraction of samples to keep out for validation\n",
    "max_epochs = 50          # maximum number of epochs to run for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.0975417378730117,\n",
       " 1: 0.11055606678059682,\n",
       " 2: 0.49152096752990665,\n",
       " 3: 0.014328907585118969,\n",
       " 4: 0.6015512028394899,\n",
       " 5: 0.41869330879453137,\n",
       " 6: 1.0,\n",
       " 7: 0.2020507427369528,\n",
       " 8: 0.32023136584724593,\n",
       " 9: 0.0984619429472854,\n",
       " 10: 0.5774944130406204,\n",
       " 11: 0.15958985145260943,\n",
       " 12: 0.1125279347968976,\n",
       " 13: 0.06441435519915867,\n",
       " 14: 0.10148547390561326,\n",
       " 15: 0.33258840541606416,\n",
       " 16: 0.15038780070987248,\n",
       " 17: 0.4878401472328119,\n",
       " 18: 0.26317865124227685,\n",
       " 19: 0.19232286052320233,\n",
       " 20: 0.30971473642697517,\n",
       " 21: 0.08847114499802813,\n",
       " 22: 0.16550545550151177,\n",
       " 23: 0.10674378861574865,\n",
       " 24: 0.10135401603785986,\n",
       " 25: 0.6331010911003023,\n",
       " 26: 0.2681740502169055,\n",
       " 27: 0.5297752070461417,\n",
       " 28: 0.2250558695937952,\n",
       " 29: 0.19666097015906403,\n",
       " 30: 0.2576574207966347,\n",
       " 31: 0.08373866175890628}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Classes are pretty imbalanced, so let's balance them out\n",
    "\n",
    "weights = []\n",
    "for i in lb.classes_:\n",
    "    weights.append(len(np.where(np.argmax(y, axis=1) == i)[0]))\n",
    "    \n",
    "max_weight = max(weights)\n",
    "weights = [1.*x/max_weight for x in weights]\n",
    "class_weight = {k: w for k, w in zip(lb.classes_, weights)}\n",
    "\n",
    "class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving checkpoints to: models/256_1_256_256_0.5_False_20_6747_64_32_sigmoid_{epoch:03d}_{val_loss:.5f}_{val_acc:.5f}_titles.h5\n"
     ]
    }
   ],
   "source": [
    "# Let's save our best current checkpoints, and stop if we haven't improved in 3 iterations w.r.t. val_acc.\n",
    "\n",
    "model_dir = 'models'\n",
    "basename = '{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}'.format(gru_dim, num_gru, embed_dim, dense_dim,  \n",
    "                dropout, bidirectional, maxlen, topn, batch_size, len(lb.classes_), gru_activation) \n",
    "filepath = os.path.join(model_dir, basename + '_{epoch:03d}_{val_loss:.5f}_{val_acc:.5f}_titles.h5')\n",
    "\n",
    "print('saving checkpoints to: {}'.format(filepath))\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True, \n",
    "                                   save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 20, 256)           1727488   \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 256)               393984    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                8224      \n",
      "=================================================================\n",
      "Total params: 2,195,488\n",
      "Trainable params: 2,195,488\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = rnn.get_training_model(topn, embed_dim, dense_dim, gru_dim, num_gru, maxlen, dropout,\n",
    "                               bidirectional, len(lb.classes_), gru_activation)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 59992 samples, validate on 6666 samples\n",
      "Epoch 1/50\n",
      "59992/59992 [==============================] - 26s - loss: 1.0102 - acc: 0.2247 - val_loss: 0.8256 - val_acc: 0.3252\n",
      "Epoch 2/50\n",
      "59992/59992 [==============================] - 26s - loss: 0.7985 - acc: 0.3463 - val_loss: 0.7441 - val_acc: 0.3740\n",
      "Epoch 3/50\n",
      "59992/59992 [==============================] - 26s - loss: 0.7267 - acc: 0.3929 - val_loss: 0.7363 - val_acc: 0.4007\n",
      "Epoch 4/50\n",
      "59992/59992 [==============================] - 26s - loss: 0.6845 - acc: 0.4209 - val_loss: 0.7296 - val_acc: 0.4055\n",
      "Epoch 5/50\n",
      "59992/59992 [==============================] - 26s - loss: 0.6544 - acc: 0.4447 - val_loss: 0.7365 - val_acc: 0.4035\n",
      "Epoch 6/50\n",
      "59992/59992 [==============================] - 26s - loss: 0.6320 - acc: 0.4600 - val_loss: 0.7425 - val_acc: 0.4109\n",
      "Epoch 7/50\n",
      "59992/59992 [==============================] - 26s - loss: 0.6111 - acc: 0.4744 - val_loss: 0.7427 - val_acc: 0.4125\n",
      "Epoch 8/50\n",
      "59992/59992 [==============================] - 26s - loss: 0.5900 - acc: 0.4868 - val_loss: 0.7581 - val_acc: 0.4142\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X, y, epochs=max_epochs, validation_split=validation_split,\n",
    "                 callbacks=[model_checkpoint, early_stopping],\n",
    "                 class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use model to remove \"hints\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start by reloading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.6 s, sys: 32 ms, total: 1.63 s\n",
      "Wall time: 1.62 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Load the source data frame\n",
    "\n",
    "filepath = '../data/title-1-True.pkl'\n",
    "\n",
    "df, keep_stops, min_sents, vocab, w2i, i2w = utils.read_dataset(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 60 ms, sys: 4 ms, total: 64 ms\n",
      "Wall time: 64.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Filter out uncommon words using parameters parsed from model name.\n",
    "\n",
    "df['encoded_text'] = utils.filter_top_words(df['encoded_text'].tolist(), topn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create input/output data\n",
    "\n",
    "X = pad_sequences(df['encoded_text'], maxlen=maxlen, value=topn, \n",
    "                  padding='post', truncating='post')\n",
    "\n",
    "Y = df['encoded_domain'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify domains with lots of \"suspicious\", ie. very high confidence, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.32 s, sys: 384 ms, total: 8.7 s\n",
      "Wall time: 7.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Get output probabilities for every observation.\n",
    "\n",
    "Ypred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a column with the highest prediction\n",
    "\n",
    "df['predicted_prob'] = np.max(Ypred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66240/66658 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "# Create a column with the predicted domain\n",
    "\n",
    "df['predicted_domain'] = model.predict_classes(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14074, 10)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pull out high-probability samples\n",
    "\n",
    "hi = pd.DataFrame(df[df['predicted_prob'] > 0.9])\n",
    "hi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ap.org</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>americanthinker.com</th>\n",
       "      <td>0.998652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>realclearpolitics.com</th>\n",
       "      <td>0.997003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rightwingnews.com</th>\n",
       "      <td>0.990662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>breitbart.com</th>\n",
       "      <td>0.591067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>westernjournalism.com</th>\n",
       "      <td>0.521429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>go.com</th>\n",
       "      <td>0.421580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>washingtonpost.com</th>\n",
       "      <td>0.243424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn.com</th>\n",
       "      <td>0.203760</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         domain\n",
       "ap.org                 1.000000\n",
       "americanthinker.com    0.998652\n",
       "realclearpolitics.com  0.997003\n",
       "rightwingnews.com      0.990662\n",
       "breitbart.com          0.591067\n",
       "westernjournalism.com  0.521429\n",
       "go.com                 0.421580\n",
       "washingtonpost.com     0.243424\n",
       "cnn.com                0.203760"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at domains that have the highest probabilities\n",
    "\n",
    "conf_thresh = 0.10  # threshold for how many high-confidence samples a domain has\n",
    "\n",
    "counts_hi = pd.DataFrame(hi['domain'].value_counts(), dtype=float) \n",
    "counts_all = pd.DataFrame(df['domain'].value_counts())\n",
    "\n",
    "# Compute the ratio of hi-conf\n",
    "for row in counts_hi.iterrows():\n",
    "    counts_hi.ix[row[0]] /= counts_all.ix[row[0]]\n",
    "\n",
    "# Sort by percent of hi-conf and truncate at threshold\n",
    "counts_hi = counts_hi.sort_values('domain', ascending=False)\n",
    "counts_hi = counts_hi[counts_hi['domain'] > conf_thresh]\n",
    "counts_hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the checkpoint you want to use and split it to allow sequential predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.48 s, sys: 820 ms, total: 2.3 s\n",
      "Wall time: 1.44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_name = 'models/256_1_256_256_0.5_False_20_6747_64_32_sigmoid_003_0.72958_0.40549_titles.h5'\n",
    "\n",
    "# Get model params from name\n",
    "gru_dim, num_gru, embed_dim, dense_dim, dropout, bidirectional, maxlen, topn, batch_size, output_size, gru_activation = \\\n",
    "  utils.parse_model_name(model_name)\n",
    "\n",
    "# Load the keras model\n",
    "model = load_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the model so you can predict token by token\n",
    "\n",
    "in_model, out_model = rnn.split_model_layers(model, topn, embed_dim, dense_dim, gru_dim, num_gru, maxlen, \n",
    "  output_size, bidirectional, gru_activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the model to identify tokens used to \"cheat\" and create blacklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hival_tokens(domain, conf_thresh=0.9, token_thresh=0.9):\n",
    "    ''' Returns a dictionary of tokens that occur almost always in a specific domain.\n",
    "    These are considered hints that the model is using to solve the bias classification\n",
    "    problem, and should be removed.\n",
    "    \n",
    "    I'm being a little sloppy here and pulling X df, *_model from global scope :P\n",
    "    \n",
    "    Args:\n",
    "        domain_thresh: threshold above which a domain prediction is considered high-confidence\n",
    "        token_thresh: threshold for considering a token to have a high confidence\n",
    "    '''\n",
    "\n",
    "    # Extract the specified domain\n",
    "    hi = pd.DataFrame(df[df['domain'] == domain])\n",
    "\n",
    "    # Only consider rows where the prediction was correct, i.e., cheating may have happened\n",
    "    hi = pd.DataFrame(hi[hi['encoded_domain'] == hi['predicted_domain']])\n",
    "    \n",
    "    # Select only high-confidence samples\n",
    "    hi = pd.DataFrame(hi[hi['predicted_prob'] > conf_thresh])\n",
    "  \n",
    "    # For each observation\n",
    "    tokens = defaultdict(float)\n",
    "    for row in hi.iterrows():\n",
    "        idx = np.where(df.index == row[0])[0][0]\n",
    "        P = sequential_pred_for_class(X, df, idx, in_model, out_model)\n",
    "        T = df['tokenized'].iloc[idx].split()\n",
    "        hival_idx = np.where(P > token_thresh)[0]\n",
    "        for i in hival_idx:\n",
    "            if i >= len(T):\n",
    "                break\n",
    "            tokens[T[i]] += 1\n",
    "  \n",
    "    # Normalize the token counts as probabilities\n",
    "    for k in tokens:\n",
    "        tokens[k] = 1. * tokens[k] / hi.shape[0]\n",
    "  \n",
    "    return hi, tokens\n",
    "\n",
    "def create_blacklist(conf_thresh=0.2, domain_prob=0.9, token_thresh=0.9, count_thresh=0.2):\n",
    "    '''This function returns a blacklist dictionary indicating which tokens are allowing\n",
    "    the model to cheat for each domain.\n",
    "    \n",
    "    Args:\n",
    "        conf_thresh: threshold for considering a prediction high-confidence\n",
    "        domain_prob: threshold for considering a domain to have a high number of hi-conf predictions\n",
    "        token_thresh: threshold for fraction of articles containing token\n",
    "        count_thresh: threshold for number of times a token must \n",
    "    '''\n",
    "    \n",
    "    # Get rows corresponding to high predicted probability\n",
    "    hi = pd.DataFrame(df[df['predicted_prob'] > conf_thresh])\n",
    "\n",
    "    # Get fraction of high-confidence samples per domain\n",
    "    counts_hi = pd.DataFrame(hi['domain'].value_counts(), dtype=float)\n",
    "    counts_all = pd.DataFrame(df['domain'].value_counts())\n",
    "    for row in counts_hi.iterrows():\n",
    "        counts_hi.ix[row[0]] /= counts_all.ix[row[0]]\n",
    "\n",
    "    # Sort the count values by domain\n",
    "    counts_hi = counts_hi.sort_values('domain', ascending=False)\n",
    "    counts_hi = counts_hi[counts_hi['domain'] > domain_prob]\n",
    "\n",
    "    # For each domain, identify tokens that give the model high confidence\n",
    "    blacklist = {}\n",
    "    for domain in counts_hi.index:\n",
    "        print(domain)\n",
    "        hi, tokens = get_hival_tokens(domain, conf_thresh, token_thresh)\n",
    "        counts = [(v, k) for k,v in tokens.iteritems()]\n",
    "        blacklist[domain] = [x[1] for x in counts if x[0] >= count_thresh]\n",
    "        print('  {}'.format(blacklist[domain]))\n",
    "\n",
    "    return blacklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "americanthinker.com\n",
      "  [u'article_NOUN', u'the_DET', u'of_ADP']\n",
      "ap.org\n",
      "  [u'press_PROPN', u'associated_PROPN']\n",
      "realclearpolitics.com\n",
      "  [u'realclearpolitic_NOUN']\n",
      "rightwingnews.com\n",
      "  [u'news_PROPN', u'right_PROPN', u'hawkins_PROPN', u'wing_PROPN', u'john_PROPN']\n",
      "cnn.com\n",
      "  [u'cnn_PROPN', u'video_PROPN']\n",
      "breitbart.com\n",
      "  [u'breitbart_PROPN']\n",
      "nytimes.com\n",
      "  []\n",
      "westernjournalism.com\n",
      "  [u'-PRON-_PRON']\n",
      "wsj.com\n",
      "  []\n",
      "theatlantic.com\n",
      "  []\n",
      "washingtonpost.com\n",
      "  [u'-PRON-_PRON', u'opinion_NOUN', u'the_DET', u'a_DET', u'trump_PROPN', u'be_VERB']\n",
      "usatoday.com\n",
      "  []\n",
      "newsmax.com\n",
      "  []\n",
      "go.com\n",
      "  [u'in_ADP', u'trump_PROPN']\n",
      "cnbc.com\n",
      "  [u'be_VERB']\n",
      "weeklystandard.com\n",
      "  []\n"
     ]
    }
   ],
   "source": [
    "blacklist = create_blacklist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'americanthinker.com': [u'article_NOUN', u'the_DET', u'of_ADP'],\n",
       " u'ap.org': [u'press_PROPN', u'associated_PROPN'],\n",
       " u'breitbart.com': [u'breitbart_PROPN'],\n",
       " u'cnbc.com': [u'be_VERB'],\n",
       " u'cnn.com': [u'cnn_PROPN', u'video_PROPN'],\n",
       " u'go.com': [u'in_ADP', u'trump_PROPN'],\n",
       " u'newsmax.com': [],\n",
       " u'nytimes.com': [],\n",
       " u'realclearpolitics.com': [u'realclearpolitic_NOUN'],\n",
       " u'rightwingnews.com': [u'news_PROPN',\n",
       "  u'right_PROPN',\n",
       "  u'hawkins_PROPN',\n",
       "  u'wing_PROPN',\n",
       "  u'john_PROPN'],\n",
       " u'theatlantic.com': [],\n",
       " u'usatoday.com': [],\n",
       " u'washingtonpost.com': [u'-PRON-_PRON',\n",
       "  u'opinion_NOUN',\n",
       "  u'the_DET',\n",
       "  u'a_DET',\n",
       "  u'trump_PROPN',\n",
       "  u'be_VERB'],\n",
       " u'weeklystandard.com': [],\n",
       " u'westernjournalism.com': [u'-PRON-_PRON'],\n",
       " u'wsj.com': []}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blacklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>label</th>\n",
       "      <th>url</th>\n",
       "      <th>domain</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>encoded_text</th>\n",
       "      <th>encoded_domain</th>\n",
       "      <th>encoded_label</th>\n",
       "      <th>predicted_prob</th>\n",
       "      <th>predicted_domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN/ORC Poll: Most Americans Want Washington C...</td>\n",
       "      <td>conservative</td>\n",
       "      <td>http://www.newsmax.com/Politics/Americans-comp...</td>\n",
       "      <td>newsmax.com</td>\n",
       "      <td>poll_NOUN most_ADJ americans_PROPN want_VERB w...</td>\n",
       "      <td>[458, 672, 198, 69, 130]</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0.354010</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Marines' Nude Photo Scandal Goes Beyond That O...</td>\n",
       "      <td>liberal</td>\n",
       "      <td>http://www.huffingtonpost.com/entry/marines-nu...</td>\n",
       "      <td>huffingtonpost.com</td>\n",
       "      <td>marines_PROPN nude_PROPN photo_PROPN scandal_N...</td>\n",
       "      <td>[1738, 1936, 415, 56, 2032, 116, 87, 507, 791,...</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.165722</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Man Survives 1,500-Foot Fall off Mountain - Br...</td>\n",
       "      <td>conservative</td>\n",
       "      <td>http://www.breitbart.com/big-government/2017/0...</td>\n",
       "      <td>breitbart.com</td>\n",
       "      <td>man_NOUN survive_VERB fall_PROPN off_ADP mount...</td>\n",
       "      <td>[51, 1218, 2506, 374, 5544, 29]</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GOP health-care bill would drop addiction trea...</td>\n",
       "      <td>liberal</td>\n",
       "      <td>https://www.washingtonpost.com/news/wonk/wp/20...</td>\n",
       "      <td>washingtonpost.com</td>\n",
       "      <td>gop_PROPN health_NOUN care_NOUN bill_NOUN woul...</td>\n",
       "      <td>[47, 72, 89, 82, 131, 305, 3865, 2176, 4881, 1...</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>0.221614</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mansfield Timberview tops Corpus Christi Memor...</td>\n",
       "      <td>conservative</td>\n",
       "      <td>http://www.washingtontimes.com/news/2017/mar/9...</td>\n",
       "      <td>washingtontimes.com</td>\n",
       "      <td>mansfield_PROPN timberview_PROPN top_VERB corp...</td>\n",
       "      <td>[2808, 5153, 4995, 4183, 2]</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0.426671</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title         label  \\\n",
       "0  CNN/ORC Poll: Most Americans Want Washington C...  conservative   \n",
       "1  Marines' Nude Photo Scandal Goes Beyond That O...       liberal   \n",
       "2  Man Survives 1,500-Foot Fall off Mountain - Br...  conservative   \n",
       "3  GOP health-care bill would drop addiction trea...       liberal   \n",
       "4  Mansfield Timberview tops Corpus Christi Memor...  conservative   \n",
       "\n",
       "                                                 url               domain  \\\n",
       "0  http://www.newsmax.com/Politics/Americans-comp...          newsmax.com   \n",
       "1  http://www.huffingtonpost.com/entry/marines-nu...   huffingtonpost.com   \n",
       "2  http://www.breitbart.com/big-government/2017/0...        breitbart.com   \n",
       "3  https://www.washingtonpost.com/news/wonk/wp/20...   washingtonpost.com   \n",
       "4  http://www.washingtontimes.com/news/2017/mar/9...  washingtontimes.com   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  poll_NOUN most_ADJ americans_PROPN want_VERB w...   \n",
       "1  marines_PROPN nude_PROPN photo_PROPN scandal_N...   \n",
       "2  man_NOUN survive_VERB fall_PROPN off_ADP mount...   \n",
       "3  gop_PROPN health_NOUN care_NOUN bill_NOUN woul...   \n",
       "4  mansfield_PROPN timberview_PROPN top_VERB corp...   \n",
       "\n",
       "                                        encoded_text  encoded_domain  \\\n",
       "0                           [458, 672, 198, 69, 130]              15   \n",
       "1  [1738, 1936, 415, 56, 2032, 116, 87, 507, 791,...              11   \n",
       "2                    [51, 1218, 2506, 374, 5544, 29]               2   \n",
       "3  [47, 72, 89, 82, 131, 305, 3865, 2176, 4881, 1...              27   \n",
       "4                        [2808, 5153, 4995, 4183, 2]              28   \n",
       "\n",
       "   encoded_label  predicted_prob  predicted_domain  \n",
       "0              0        0.354010                 6  \n",
       "1              1        0.165722                 6  \n",
       "2              0        0.999998                 2  \n",
       "3              1        0.221614                 6  \n",
       "4              0        0.426671                 4  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter out cheat words and re-encode text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dg = df.copy()\n",
    "df = dg.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "for domain in blacklist.keys():\n",
    "    domain_index = df[df['domain'] == domain].index\n",
    "    data = df.ix[domain_index, 'tokenized'].tolist()\n",
    "    for ix, d in enumerate(data):\n",
    "        data[ix] = ' '.join([x for x in d.split() if x not in blacklist[domain]])\n",
    "    df.loc[domain_index, 'tokenized'] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['encoded_text'] = df['tokenized'].map(lambda x: [w2i[y] for y in x.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote to ../data/title-1-True-clean.pkl\n",
      "CPU times: user 1.69 s, sys: 16 ms, total: 1.71 s\n",
      "Wall time: 1.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "OUTPUT_FILE = '../data/title-{}-{}-clean.pkl'.format(min_sents, keep_stops)\n",
    "\n",
    "_ = utils.write_dataset(OUTPUT_FILE, df, keep_stops, min_sents, vocab, w2i, i2w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
