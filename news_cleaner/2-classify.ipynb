{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we'll do further processing of the input text (in the spirit of hyperparameter tuning, rather than cleaning etc).  We'll then build and train a simple RNN classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, log_loss\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import utils\n",
    "import rnn\n",
    "\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%autoreload\n",
    "#import rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.53 s, sys: 40 ms, total: 1.57 s\n",
      "Wall time: 1.57 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#filepath = '../data/data-False-3.pkl'\n",
    "filepath = '../data/data-True-0.pkl'\n",
    "\n",
    "df, keep_stops, min_sents, vocab, w2i, i2w = utils.read_dataset(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limit vocab size, pad sequences, and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxlen = 20      # Max number of tokens in input sequence\n",
    "frac_drop = 0.0  # Fraction of tokens to randomly drop from input sequences\n",
    "topn = 20000     # Keep only the top n words in vocabulary\n",
    "test_size = 0.0   # Fraction of samples to keep out of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 156 ms, sys: 16 ms, total: 172 ms\n",
      "Wall time: 155 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Filter out uncommon words.\n",
    "\n",
    "df['encoded_text'] = utils.filter_top_words(df['encoded_text'].tolist(), topn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_train, df_valid = train_test_split(df, test_size=test_size)\n",
    "# df_valid, df_test = train_test_split(df_valid, test_size=0.5)\n",
    "\n",
    "# df_train.shape, df_valid.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((66659, 20), (0, 20), (66659,), (0,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = pad_sequences(df['encoded_text'], maxlen=maxlen, value=topn,  \n",
    "                        padding='post', truncating='post') \n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_train, df['encoded_label'], test_size=test_size)\n",
    "\n",
    "X_train.shape, X_test.shape, Y_train.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66659, 32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb = LabelBinarizer()\n",
    "Y_train = lb.fit_transform(Y_train)\n",
    "#Y_test = lb.transform(Y_test)\n",
    "\n",
    "Y_train.shape #Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Delete the dataframe, we are done with it for now!\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model\n",
    "\n",
    "This is an important part, so I'll be explicit here rather than hiding things in `utils :)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed_dim = 128         # embedding dimension for word vecs\n",
    "gru_dim = 64           # dimension of GRU layers\n",
    "num_gru = 1\n",
    "bidirectional = False    # whether to use bidirectional\n",
    "dense_dim = 64         # dimensionality of dense layer\n",
    "dropout = 0.5       # dropout ratio\n",
    "batch_size = 64         # batch size\n",
    "validation_split = 0.1 # Fraction of samples to keep out for validation\n",
    "max_epochs = 50         # maximum number of epochs to run for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.09752891692954785,\n",
       " 1: 0.11054153522607782,\n",
       " 2: 0.4914563617245005,\n",
       " 3: 0.014327024185068349,\n",
       " 4: 0.601472134595163,\n",
       " 5: 0.41863827549947424,\n",
       " 6: 1.0,\n",
       " 7: 0.2020241850683491,\n",
       " 8: 0.32018927444794953,\n",
       " 9: 0.09844900105152471,\n",
       " 10: 0.5774185068349106,\n",
       " 11: 0.15956887486855942,\n",
       " 12: 0.11251314405888538,\n",
       " 13: 0.06440588853838065,\n",
       " 14: 0.10147213459516298,\n",
       " 15: 0.3325446898002103,\n",
       " 16: 0.15036803364879076,\n",
       " 17: 0.4877760252365931,\n",
       " 18: 0.2631440588853838,\n",
       " 19: 0.1922975814931651,\n",
       " 20: 0.3096740273396425,\n",
       " 21: 0.08845951629863302,\n",
       " 22: 0.16548370136698212,\n",
       " 23: 0.10672975814931651,\n",
       " 24: 0.10134069400630914,\n",
       " 25: 0.6330178759200841,\n",
       " 26: 0.26813880126182965,\n",
       " 27: 0.5297055730809674,\n",
       " 28: 0.22502628811777076,\n",
       " 29: 0.19663512092534174,\n",
       " 30: 0.2576235541535226,\n",
       " 31: 0.08372765509989485}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# About 43% of articles are conservative, so let's weight samples accordingly\n",
    "\n",
    "weights = []\n",
    "for i in lb.classes_:\n",
    "    weights.append(len(np.where(np.argmax(Y_train, axis=1) == i)[0]))\n",
    "    \n",
    "max_weight = max(weights)\n",
    "weights = [1.*x/max_weight for x in weights]\n",
    "class_weight = {k: w for k, w in zip(lb.classes_, weights)}\n",
    "\n",
    "class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving checkpoints to: models/64_128_64_0.5_False_20_20000_0.0_64_0.0_1_32_titles_{epoch:03d}_{val_loss:.5f}_{val_acc:.5f}_titles.h5\n"
     ]
    }
   ],
   "source": [
    "# Let's save our best current checkpoints, and stop if we haven't improved in 3 iterations w.r.t. val_acc.\n",
    "\n",
    "model_dir = 'models'\n",
    "basename = '{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_titles'.format(gru_dim, embed_dim, dense_dim, dropout, bidirectional,\n",
    "                                               maxlen, topn, test_size, batch_size, frac_drop, num_gru, len(lb.classes_)) \n",
    "filepath = os.path.join(model_dir, basename + '_{epoch:03d}_{val_loss:.5f}_{val_acc:.5f}_titles.h5')\n",
    "\n",
    "print('saving checkpoints to: {}'.format(filepath))\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True, \n",
    "                                   save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 20, 128)           2560128   \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 64)                37056     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "=================================================================\n",
      "Total params: 2,603,424\n",
      "Trainable params: 2,603,424\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = rnn.get_training_model(topn, embed_dim, dense_dim, gru_dim, num_gru, maxlen, dropout,\n",
    "                               output_size=len(lb.classes_), bidirectional=bidirectional)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 53327 samples, validate on 13332 samples\n",
      "Epoch 1/50\n",
      "53327/53327 [==============================] - 25s - loss: 1.0573 - acc: 0.1873 - val_loss: 0.8901 - val_acc: 0.2538\n",
      "Epoch 2/50\n",
      "53327/53327 [==============================] - 24s - loss: 0.8704 - acc: 0.2882 - val_loss: 0.8005 - val_acc: 0.3397\n",
      "Epoch 3/50\n",
      "53327/53327 [==============================] - 24s - loss: 0.7846 - acc: 0.3405 - val_loss: 0.7776 - val_acc: 0.3434\n",
      "Epoch 4/50\n",
      "53327/53327 [==============================] - 24s - loss: 0.7353 - acc: 0.3628 - val_loss: 0.7729 - val_acc: 0.3523\n",
      "Epoch 5/50\n",
      "53327/53327 [==============================] - 24s - loss: 0.6971 - acc: 0.3873 - val_loss: 0.7678 - val_acc: 0.3622\n",
      "Epoch 6/50\n",
      "53327/53327 [==============================] - 24s - loss: 0.6634 - acc: 0.4104 - val_loss: 0.7767 - val_acc: 0.3769\n",
      "Epoch 7/50\n",
      "53327/53327 [==============================] - 25s - loss: 0.6349 - acc: 0.4383 - val_loss: 0.7791 - val_acc: 0.3864\n",
      "Epoch 8/50\n",
      "53327/53327 [==============================] - 25s - loss: 0.6058 - acc: 0.4597 - val_loss: 0.7879 - val_acc: 0.3936\n",
      "Epoch 9/50\n",
      "53327/53327 [==============================] - 25s - loss: 0.5833 - acc: 0.4780 - val_loss: 0.8063 - val_acc: 0.3951\n",
      "Epoch 10/50\n",
      "53327/53327 [==============================] - 25s - loss: 0.5636 - acc: 0.4919 - val_loss: 0.8082 - val_acc: 0.4004\n",
      "Epoch 11/50\n",
      "53327/53327 [==============================] - 25s - loss: 0.5465 - acc: 0.5047 - val_loss: 0.8294 - val_acc: 0.4018\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X_train, Y_train, epochs=max_epochs, validation_split=0.2,\n",
    "                 callbacks=[model_checkpoint, early_stopping],\n",
    "                 class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
