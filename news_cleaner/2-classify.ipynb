{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we'll do further processing of the input text (in the spirit of hyperparameter tuning, rather than cleaning etc).  We'll then build and train a simple RNN classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, log_loss\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import utils\n",
    "import rnn\n",
    "\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%autoreload\n",
    "#import rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.53 s, sys: 28 ms, total: 1.56 s\n",
      "Wall time: 1.56 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#filepath = '../data/data-False-3.pkl'\n",
    "filepath = '../data/data-True-0.pkl'\n",
    "\n",
    "df, keep_stops, min_sents, vocab, w2i, i2w = utils.read_dataset(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limit vocab size, pad sequences, and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxlen = 32      # Max number of tokens in input sequence\n",
    "frac_drop = 0.0  # Fraction of tokens to randomly drop from input sequences\n",
    "topn = 20000     # Keep only the top n words in vocabulary\n",
    "test_size = 0.0   # Fraction of samples to keep out of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 120 ms, sys: 16 ms, total: 136 ms\n",
      "Wall time: 122 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Filter out uncommon words.\n",
    "\n",
    "df['encoded_text'] = utils.filter_top_words(df['encoded_text'].tolist(), topn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_train, df_valid = train_test_split(df, test_size=test_size)\n",
    "# df_valid, df_test = train_test_split(df_valid, test_size=0.5)\n",
    "\n",
    "# df_train.shape, df_valid.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((66659, 32), (0, 32), (66659,), (0,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = pad_sequences(df['encoded_text'], maxlen=maxlen, value=topn,  \n",
    "                        padding='post', truncating='post') \n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_train, df['encoded_label'], test_size=test_size)\n",
    "\n",
    "X_train.shape, X_test.shape, Y_train.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66659, 32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb = LabelBinarizer()\n",
    "Y_train = lb.fit_transform(Y_train)\n",
    "#Y_test = lb.transform(Y_test)\n",
    "\n",
    "Y_train.shape #Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Delete the dataframe, we are done with it for now!\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model\n",
    "\n",
    "This is an important part, so I'll be explicit here rather than hiding things in `utils :)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed_dim = 256         # embedding dimension for word vecs\n",
    "gru_dim = 256           # dimension of GRU layers\n",
    "num_gru = 1\n",
    "bidirectional = True    # whether to use bidirectional\n",
    "dense_dim = 256         # dimensionality of dense layer\n",
    "dropout = 0.5       # dropout ratio\n",
    "batch_size = 64         # batch size\n",
    "validation_split = 0.1 # Fraction of samples to keep out for validation\n",
    "max_epochs = 50         # maximum number of epochs to run for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.09752891692954785,\n",
       " 1: 0.11054153522607782,\n",
       " 2: 0.4914563617245005,\n",
       " 3: 0.014327024185068349,\n",
       " 4: 0.601472134595163,\n",
       " 5: 0.41863827549947424,\n",
       " 6: 1.0,\n",
       " 7: 0.2020241850683491,\n",
       " 8: 0.32018927444794953,\n",
       " 9: 0.09844900105152471,\n",
       " 10: 0.5774185068349106,\n",
       " 11: 0.15956887486855942,\n",
       " 12: 0.11251314405888538,\n",
       " 13: 0.06440588853838065,\n",
       " 14: 0.10147213459516298,\n",
       " 15: 0.3325446898002103,\n",
       " 16: 0.15036803364879076,\n",
       " 17: 0.4877760252365931,\n",
       " 18: 0.2631440588853838,\n",
       " 19: 0.1922975814931651,\n",
       " 20: 0.3096740273396425,\n",
       " 21: 0.08845951629863302,\n",
       " 22: 0.16548370136698212,\n",
       " 23: 0.10672975814931651,\n",
       " 24: 0.10134069400630914,\n",
       " 25: 0.6330178759200841,\n",
       " 26: 0.26813880126182965,\n",
       " 27: 0.5297055730809674,\n",
       " 28: 0.22502628811777076,\n",
       " 29: 0.19663512092534174,\n",
       " 30: 0.2576235541535226,\n",
       " 31: 0.08372765509989485}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# About 43% of articles are conservative, so let's weight samples accordingly\n",
    "\n",
    "weights = []\n",
    "for i in lb.classes_:\n",
    "    weights.append(len(np.where(np.argmax(Y_train, axis=1) == i)[0]))\n",
    "    \n",
    "max_weight = max(weights)\n",
    "weights = [1.*x/max_weight for x in weights]\n",
    "class_weight = {k: w for k, w in zip(lb.classes_, weights)}\n",
    "\n",
    "class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving checkpoints to: models/256_256_256_0.5_True_32_20000_0.0_64_0.0_1_32_titles_{epoch:03d}_{val_loss:.5f}_{acc:.5f}_titles.h5\n"
     ]
    }
   ],
   "source": [
    "# Let's save our best current checkpoints, and stop if we haven't improved in 3 iterations w.r.t. val_acc.\n",
    "\n",
    "model_dir = 'models'\n",
    "basename = '{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_titles'.format(gru_dim, embed_dim, dense_dim, dropout, bidirectional,\n",
    "                                               maxlen, topn, test_size, batch_size, frac_drop, num_gru, len(lb.classes_)) \n",
    "filepath = os.path.join(model_dir, basename + '_{epoch:03d}_{val_loss:.5f}_{val_acc:.5f}_titles.h5')\n",
    "\n",
    "print('saving checkpoints to: {}'.format(filepath))\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True, \n",
    "                                   save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 32, 256)           5120256   \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 512)               787968    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                8224      \n",
      "=================================================================\n",
      "Total params: 6,047,776\n",
      "Trainable params: 6,047,776\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = rnn.get_training_model(topn, embed_dim, dense_dim, gru_dim, num_gru, maxlen, dropout,\n",
    "                               output_size=len(lb.classes_), bidirectional=bidirectional)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 53327 samples, validate on 13332 samples\n",
      "Epoch 1/50\n",
      "53327/53327 [==============================] - 66s - loss: 0.8825 - acc: 0.3074 - val_loss: 0.7219 - val_acc: 0.4072\n",
      "Epoch 2/50\n",
      "53327/53327 [==============================] - 66s - loss: 0.6737 - acc: 0.4390 - val_loss: 0.6830 - val_acc: 0.4278\n",
      "Epoch 3/50\n",
      "53327/53327 [==============================] - 66s - loss: 0.5713 - acc: 0.5066 - val_loss: 0.6741 - val_acc: 0.4474\n",
      "Epoch 4/50\n",
      "53327/53327 [==============================] - 66s - loss: 0.5025 - acc: 0.5556 - val_loss: 0.7080 - val_acc: 0.4545\n",
      "Epoch 5/50\n",
      "53327/53327 [==============================] - 66s - loss: 0.4496 - acc: 0.5898 - val_loss: 0.7280 - val_acc: 0.4518\n",
      "Epoch 6/50\n",
      "53327/53327 [==============================] - 66s - loss: 0.4103 - acc: 0.6174 - val_loss: 0.7799 - val_acc: 0.4570\n",
      "Epoch 7/50\n",
      "53327/53327 [==============================] - 66s - loss: 0.3758 - acc: 0.6434 - val_loss: 0.8540 - val_acc: 0.4572\n",
      "Epoch 8/50\n",
      "53327/53327 [==============================] - 66s - loss: 0.3521 - acc: 0.6594 - val_loss: 0.8736 - val_acc: 0.4602\n",
      "Epoch 9/50\n",
      "53327/53327 [==============================] - 66s - loss: 0.3281 - acc: 0.6766 - val_loss: 0.9287 - val_acc: 0.4572\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X_train, Y_train, epochs=max_epochs, validation_split=0.2,\n",
    "                 callbacks=[model_checkpoint, early_stopping],\n",
    "                 class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple evaluation metrics on hold-out test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the best checkpoint\n",
    "\n",
    "model_name = 'models/64_128_64_0.5_True_20_10000_0.1_64_0.1_1_titles_001_0.53464_0.71825_titles.h5'\n",
    "model = load_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
